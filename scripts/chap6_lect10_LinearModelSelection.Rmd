Model Selection
===============

This is an R Markdown format document, a simple
syntax for authoring web pages. It has some 
simple syntax rules and makes for a nice, clean
way of publicly distributing an analysis.
<!-- new comment syntax (html) -->


<!-- chop code into runable chunks enclosed in ``` symbols -->
```{r}
library(ISLR)
summary(Hitters)
```
There are missing values in Salary here, so before we
proceed, we will remove them.

<!-- short snippet of code to remove NA values -->
```{r}
Hitters=na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```
<!-- first line removes NA in Hitters dataframe;
     second line tests for NA values by asking 
     to return sum of NA with dataframe Hitters -->


Best Subset regression
----------------------
We will use the package 'leaps' to evaluate all
the best-subset models.

```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
```

<!-- regsubsets uses Salary as response with all
     variables from dataframe Hitters as predictors;
     performs best subset regression and returns
     table with starred values in columns of 
     predictors that are part of the best subset
     for a given subset size -->
     
It gives by default best subsets up to size 8; we 
can increase that to 19, i.e., all the variables
in the dataframe Hitters

```{r}
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary) # gives names of summary output
# names includes RSS, C_p, BIC, etc.
plot(reg.summary$cp,xlab="Number of Variables",ylab='Cp') # plot to see minimized Cp (~10 variables)
which.min(reg.summary$cp) # returns number of variables that minimizes Cp; yep, it's 10
points(10,reg.summary$cp[10],pch=20,col='red') # plot a red dot on minimized Cp value at 10 variables
```

There is a separate plot method for `regsubsets` object

```{r}
plot(regfit.full,scale="Cp") 
# plots an ugly black/white heatmap of Cp showing the value of Cp relative to which variables are included in the model (black = included, white = excluded)
coef(regfit.full,10)
# gives the fit coefficients for all variables (10) included in the model for minimized Cp 
```

<!-- Use the Knit>Knit to HTML option from the 
    menu bar to see a preview of the markdown
    format for a website -->

Forward Stepwise Selection
--------------------------
Here we will use the `regsubsets` function but specify the `method="forward"` option:

```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp") # very similar to best subset selection for the given dataframe Hitters
```
<!-- regsubsets with forward stepwise selection
     now tests one variable at a time for a given
     subset size and finds the best variable;
     returns table with starred columns for a 
     given subset size always including the 
     previously found best variable -->

Model Selection Using a Validation Set
--------------------------------------
Here we make a training and validation set so that
we can choose a good subset model. (This approach
differs slightly from the book.)

```{r}
dim(Hitters)
set.seed(1) # set random number seed to 1 for reproducability in this demonstration
train=sample(seq(263),180,replace=FALSE) # sample 180 observations out of sequence of 263
# uses about half (~180 based on dim info)
# of the data for training set, the other half
# is reserved for validation
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19,method="forward")
```

Now we make predictions on the observations not used for training. There are 19 models based on the variables in the Hitters dataframe, so we set up vectors to record the errors on the models. A little work is required since there is no predict method for `regsubsets`.

```{r}
val.errors=rep(NA,19)
x.test=model.matrix(Salary~.,data=Hitters[-train,]) # create x matrix to record model errors
# now rather than indexing by train, we index
# by -train, which is a nice function in R to 
# index by excluding the observations indexed by train
for(i in 1:19){
  coefi=coef(regfit.fwd,id=i) # calculate coefficient vector for each model size 1-19 where i is the size
  # coefi only has subset of variables that are used in ith model
  # to get the right elements of x.test we must index the columns by the names that exist on coefi vector
  pred=x.test[,names(coefi)]%*%coefi # subset of columns of x.text that correspond to subset of variables in ith model
  # %*% is matrix multiplier by coefi vector
  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2) # mean squared error on the prediction
}
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400),pch=19,type='b')
# minimum here seems to be at 5 variables, different than
# the Cp minimum at 10 variables from previous section
points(sqrt(regfit.fwd$rss[-1]/180),col='blue',pch=19,type='b')
# plot root mean residual sum of squares (RSS) for model
# remove first (null) model at index 1
# naturally, it will be decreasing with increasing
# model size since more variables give a better fit (overfitting)
legend("topright",legend=c("Training","Validation"),col=c('blue','black'),pch=19)
```

As we expect, the training error goes down monotonically
as the model gets bigger, but not so for the validation
error.

Creating our own predict method for `regsubsets` will make future work less tedious.

```{r}
# function make predictions from training model
# for regsubsets method
predict.regsubsets=function(object,newdata,id,...){
  # object that we want to predict from (model)
  # newdata used to make prediction (validation set)
  # id of model
  form=as.formula(object$call[[2]])
  # regsubsets method has component "call" which
  # is the call that was used to create it
  # call component contains the formula used to 
  # create the regsubsets method, which can be
  # extracted as above
  mat=model.matrix(form,newdata)
  # make a model matrix from the extracted formula
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
  # extract the model coefficients and perform
  # matrix multiplier
}
```


Model Selection by Cross Validation
-----------------------------------
We will do 10-fold cross-validation in this section.

```{r}
set.seed(1)
folds=sample(rep(1:10,length=nrow(Hitters))) 
# folds are randomly sampled indices from 1-10
# over all rows in Hitters dataframe
folds # shows folds vector
table(folds)
# first row shows number from 1-10
# second row shows how many times it appears in folds array
cv.errors=matrix(NA,10,19)
# create matrix to store CV errors

# nested for loops for cross-validation
for(k in 1:10){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
  # loop through regsubsets with forward stepwise method
  # and return best-fit on training data from Hitters
  # indexed where folds are not equal to k;
  # this takes 10 different subsets of the data
  # from Hitters as cross-validation training sets

  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==k,],id=i)
    cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2) # mean squared error on prediction
  }
  # use predict function previously defined for
  # regsubsets method to calculate predictions
  # for Hitters where folds is equal to k 
  # (data not used in training set)
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
# now apply root mean squared error calculation 
# to the error matrix;
# take column means from error matrix since it has
# 10 rows and each row was mean squared error for
# the kth fold and we want mean error over all k folds
plot(rmse.cv,pch=19,type='b')
# cross-validation model selection seems to favor
# models with 11 or 12 variables
```

Model Selection by Cross Validation
-----------------------------------
We will use the package `glmnet`, which does not
use the model formula language. Therefore, we need
to set up an `x` and `y` to define the predictors
vector and the response vector.

```{r}
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters) # x values are all variables excluding NewLeague variable at the end
y=Hitters$Salary # y values are Salary from dataframe Hitters
```

First we will fit a ridge-regression model, which is 
achieved by calling `glmnet` with `alpha-0`. Here
`alpha` is the parameter that defines the penalty 
for model complexity. It is in the range 0-1 with 
`alpha=0` the ridge penalty and `alpha=1` the lasso
penalty (see help file). There is also a cv.glmnet
function which performs cross-validation for us.

```{r}
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge,xvar='lambda',label=TRUE)
# plot shows that as model complexity
# increases (more predictors) then coefficients
# tend toward zero

# in ridge regression there is a penalty on the
# sum of squares of the coefficients;
# larger lambda (see lectures) means smaller coefficients

# NOTE: left-most part of plot where lambda=0
# gives results for coefficients equal to 
# ordinary least squares fit

cv.ridge=cv.glmnet(x,y,alpha=0)
# cv.glmnet gives default 10-fold cross validation
plot(cv.ridge)
# plot of mean squared error vs. lambda shows
# vertical lines minimum and at 1 standard error from minimum (slightly restricted model that performs as well as best model)
```

Now we fit a lasso model; we switch to `alpha=1`

```{r}
fit.lasso=glmnet(x,y,alpha=1)
plot(fit.lasso,xvar='lambda',label=TRUE)
# in lasso there is a penalty on the
# sum of absolute value of the coefficients;
# larger lambda (see lectures) means smaller
# AND fewer coefficients

# penalizing absolute value allows coefficients
# to become exactly zero, rather than close
# to zero as in the ridge method

plot(fit.lasso,xvar='dev',label=TRUE)
# can also plot coefficients as fraction 
# of deviance explained (i.e., fraction of 
# R^2 explained in the case of regression);

# NOTE: at the right of the plot there
# is only a small increase in R^2 (0.4-->0.5)
# but the coefficients grow very large, 
# suggesting that the model is overfitting

cv.lasso=cv.glmnet(x,y,alpha=1)
plot(cv.lasso)
# again, vertical lines at MSE minimum and
# 1 standard error away
coef(cv.lasso)
# coefficient extraction for best-fit model
# returns the model with fewer variables
# from the previous plot
```

Suppose we want to use our earlier training/validation
division to select `lambda` for the lasso. It's easy.

```{r}
lasso.tr=glmnet(x[train,],y[train])
lasso.tr
# prints model info, giving as columns:
# number of non-zero coefficients Df
# percent of deviance explained (R^2) %Dev
# value of Lambda
pred=predict(lasso.tr,x[-train,])
# make predictions using lasso.tr training set
# on x indexed over left-out data
dim(pred)
# we have 83 observations in observation set
# and 89 values of lambda
rmse=sqrt(apply((y[-train]-pred)^2,2,mean))
# here we are applying the pred matrix of 83x89 to a 
# vector y[-train] of length 83
# in line y[-train]-pred, the vector y[-train] is
# recycled 89 times to make the calculation
# and results in 83x89 matrix
plot(log(lasso.tr$lambda),rmse,type='b',xlab="Log(lambda)")
# we see a nice minimum in RMSE for Log(lambda)
# around 5, with underfitting to the left and 
# overfitting to the right
lam.best=lasso.tr$lambda[order(rmse)[1]]
# extract the best lambda value by calling
# component lambda from glmnet object and 
# indexing it by the order of the RMSE
lam.best
coef(lasso.tr,s=lam.best)
# find coefficients for model with best lambda
# dots in output correspond to zero-value coefficients
```
