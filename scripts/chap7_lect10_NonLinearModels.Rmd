Nonlinear Models
===========================================
Here we explore the use of nonlinear models
using some tools in R

```{r}
require(ISLR)
attach(Wage)
```

Polynomials
------------------------

First we will use polynomials, focusing on a 
single preditor (age):

```{r}
fit=lm(wage~poly(age,4),data=Wage)
# lm polynomial fit with degree 4 with wage as response;
# poly is part of R built-in formula language
summary(fit)
# coefficients from t-statistic probability shows
# that fits 2 degree terms are very significant
# with 3rd degree term less significant and 4th 
# degree term not significant; 
# cubic polynomical fit is sufficient
```

The `poly()` function generates a basis of *orthogonal polynomials*. However, we are usually not interested in the specifics of each degree coefficient, as this is only useful for very small data sets with variables that are uncorrelated. Let's make a plot of the function along with standard errors of the fit.

```{r fig.width=7, fig.height=6}
agelims=range(age) # gives min,max ranges of age
age.grid=seq(from=agelims[1],to=agelims[2]) # create sequence of numbers from min of age to max of age
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
# make predictions for sequence of ages using the polynomical fit with standard errors
se.bands=cbind(preds$fit+2*preds$se,preds$fit-2*preds$se)
# create standard error bands as as matrix with two columns using cbind
# use bands with ranges +/- 2*standard error calculated from preds
plot(age,wage,col='darkgrey') # plot data
lines(age.grid,preds$fit,lwd=2,col='blue') # plot fit
matlines(age.grid,se.bands,col='blue',lty=2) # plot SE bands
```

There are more direct ways of fitting a polynomial function in R. For example:

```{r}
fita=lm(wage~age+I(age^2)+I(age^3)+I(age^4),data=Wage)
summary(fita)
# will give different p values for degree coefficients
# compared to the previous method, but again we are not
# so interested in degree coefficients but the goodness
# of fit for the polynomial function, which is the same
```

Here `I()` is a *wrapper* function; we need it because `age^2` means something very different to the formula language while `I(age^2)` is protected from formula language interpretation.
The coefficients for this new method are different, but the polynomial fit remains the same!

```{r}
plot(fitted(fit),fitted(fita))
# above plot shows the fitted points from the first fit
# versus the fitted points from the second fit, which
# reveals that the fits are indeed equal
```

However, by using the *orthogonal polynomials* in this simple way, it turns out that we can separately test for each coefficient. If we look again at the summary of `fit` we can see that linear, quadratic, and cubic terms are significant, but not the quartic term.

```{r}
summary(fit)
```

This only works with linear regression with a single predictor. In general, we use `anova()` as this next example demonstrates with the nested sequence of models increasing in complexity.

```{r}
fita=lm(wage~education,data=Wage)
fitb=lm(wage~education+age,data=Wage)
fitc=lm(wage~education+poly(age,2),data=Wage)
fitd=lm(wage~education+poly(age,3),data=Wage)
anova(fita,fitb,fitc,fitd)
# output of anova shows significance test of each model
# looking at the significance of the models (right column)
# it appears that age as a linear predicitor (Model 2) 
# is significant when comparing wage and education,
# age^2 is also significant in the model, but age^3 is not
```

### Polynomial logistic regression

Now we fit a logistic regression model to a binary response variable constructed from `wage`. We code the big earners (`>250K`) as 1 and low earners (`<250K`) as 0.

```{r}
fit=glm(I(wage>250)~poly(age,3),data=Wage,family=binomial)
# can do logistic regression on the fly with glm
# I() wrapper function with conditional on wage evaluates to T/F function
# which is then coded as a 0,1 binary variable for glm
summary(fit)
# because glm involves weights on observations, the
# polynomial fit is no longer orthogonal and we get
# different values for the coefficients
# with glm would have to separately test polynomials
# of different degrees (e.g., 1 then 2 then 3) and
# compare directly to see what degree polynomial is best
preds=predict(fit,list(age=age.grid),se=T)
se.bands=preds$fit + cbind(fit=0,lower=-2*preds$se,upper=2*preds$se)
# different syntax for SE bands here: want predictions
# from fit to be a center of bands, so start with that
# vector as definition for bands, then define cbind
# centered at 0 with upper/lower limit which gives 3xn
# matrix; then fit vector is simply added to all three
# columns and replicated n times
se.bands[1:5,]
```

We have done the computations on the logit scale. `predict` fits the linear predictor, which is on the logit scale. We're usually more iterested in the predictions on the probability scale. To transform we need to apply inverse logit mapping:
$$p=\frac{e^\eta}{1+e^\eta}.$$
We can do this simultaneously for all three columns of `se.bands`:

```{r}
prob.bands=exp(se.bands)/(1+exp(se.bands))
matplot(age.grid,prob.bands,col='blue',lwd=c(2,1,1),lty=c(1,2,2),type='l',ylim=c(0,0.1))
points(jitter(age),I(wage>250)/10,pch="|",cex=0.5)
# jitter function adds a bit of uniform random noise
# to each vector in age
# (scale for 0,1 wage binary is limited to fit on plot)
```


Splines
---------------------------
Splines are more flexible than polynomials, but the idea is rather similar. Here we will explore cubic splines.

```{r}
require(splines)
fit1=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
# fit cubic spline (bs) with 3 knots defined
# cubic polynomials in each of the knot regions and 
# continuous in the 1st and 2nd derivative
plot(age,wage,col='darkgrey')
lines(age.grid,predict(fit1,list(age=age.grid)),col='darkgreen',lwd=2)
abline(v=c(25,40,60),lty=2,col='darkgreen')
```

The smoothing spline does not require knot selection as it in a sense defines knots everywhere, but it does have a smoothing parameter. The smoothing parameter can be conveniently specified via the effective degrees of freedom `df`.

```{r}
# replot previous fits
plot(age,wage,col='darkgrey')
lines(age.grid,predict(fit1,list(age=age.grid)),col='darkgreen',lwd=2)
abline(v=c(25,40,60),lty=2,col='darkgreen')
fit2=smooth.spline(age,wage,df=16)
# smoothing spline with 16 effective degrees of freedom
# given age as predictor and wage as response
lines(fit2,col='red',lwd=2)
# plot the smoothing spline
```

Alternatively we can use leave-one-out cross-validation (LOOCV) to select the smoothing parameter automatically.

```{r}
# replot data
plot(age,wage,col='darkgrey')
lines(age.grid,predict(fit1,list(age=age.grid)),col='darkgreen',lwd=2)
abline(v=c(25,40,60),lty=2,col='darkgreen')
lines(fit2,col='red',lwd=2)
fit3=smooth.spline(age,wage,cv=T)
# warns that there are many non-unique x values
# but that is fine and expected for our data set
lines(fit3,col='purple',lwd=2)
# pretty close to the previous fits
fit
# print out gives ~6.8 degrees of freedom
# recall effective degrees are freedom are really
# just a 'roughness' parameter for the smoothing spline
# prevous bs() fit had 6 parameters from 4 knots, so 
# 6 degrees of freedom
```


Generalized Additive Models
-----------------------------

So far we have focused on fitting models with mostly single non-linear terms. We would like to fit a model with one single predictor but in reality we have lots of predictors. The `gam` package makes it easier to work with multiple non-linear terms. It also knows how to plot these functions and their standard errors.

```{r fig.width=10, fig.height=5}
require(gam)
gam1=gam(wage~s(age,df=4)+s(year,df=4)+education,data=Wage)
# s is a special function known by gam with calls
# a smoothing spline and df;
# response wage with smooth term in age with 4 deg of 
# freedom, smooth term in year with 4 deg of freedom,
# and education as linear terms which will be fit 
# by constants on dummy variables for each of the 
# education terms
par(mfrow=c(1,3))
plot(gam1,se=T)
# plots each of the terms and their standard errors
# produced by the GAM fit
gam2=gam(I(wage>250)~s(age,df=4)+s(year,df=4)+education,data=Wage)
# gam also works for logistic regression
# again convert wage response to binary 0,1 with 250K cut
plot(gam2,se=T)
# note: very large standard errors here
plot(gam2,se=F)
```

Let's see if we need a non-linear term for years. Can test this with nested sequence of models increasing in complexity as done before.

```{r}
gam2a=gam(I(wage>250)~s(age,df=4)+year+education,data=Wage,family=binomial)
# create gam fit with linear term for year
anova(gam2a,gam2,test="Chisq")
# test linear year vs non-linear year with anova
# can test further to see if any year term is needed at all
gam2b=gam(I(wage>250)~s(age,df=4)+education,data=Wage,family=binomial)
anova(gam2b,gam2a,gam2,test="Chisq")
# appears that non-linear term for year is needed 
# NOTE: this is VERY different than the results from the
# online lectures.. updates to Wage dataframe or gam package (v1.16 here vs v1.09 in lectures)??
```
One nice feature of the `gam` package is that it can plot functions nicely, even for model fit by the `lm` and `glm` methods.

```{r}
par(mfrow=c(1,3))
lm1=lm(wage~ns(age,df=4)+ns(year,df=4)+education,data=Wage)
# fit linear model function with natural splines
# very similar to call to gam method
# can also use gam to plot the model
plot.Gam(lm1,se=T)
# note: Gam must be capilatized for this function
# to work (different from online lectures gam v1.09)
```


